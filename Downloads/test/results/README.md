# Results

This directory contains evaluation submissions from team members.

## Summary Table

_Updated as submissions come in._

| Evaluator | Use Case | Claude Code | Copilot | Winner | Date |
|-----------|----------|:-----------:|:-------:|--------|------|
| _(example)_ | Build REST API | 34/40 | 28/40 | Claude Code | 2025-02-15 |

## How to Read Results

Each submission in `submissions/` follows the [scoring template](../evaluation-framework/scoring-template.md) and includes:

- **Scorecard**: 1-5 ratings across 8 metrics for both tools
- **Timing data**: How long each tool took
- **Detailed observations**: What worked, what didn't, with code examples
- **Verdict**: Which tool the evaluator preferred for that task and why

## Trends to Watch

As we accumulate submissions, look for patterns:

- **Does one tool dominate a specific category?** (e.g., always wins on debugging)
- **Dev vs DS differences**: Does the winner change depending on the domain?
- **Experience level effects**: Do more experienced users see different results?
- **Task complexity effects**: Does one tool scale better to harder tasks?

## Adding Your Results

See [CONTRIBUTING.md](../CONTRIBUTING.md) for instructions. Drop your completed scoring template into the `submissions/` folder and submit a PR.
